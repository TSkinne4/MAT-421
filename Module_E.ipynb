{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQgZwZ/targp1Wm1BDVF5c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TSkinne4/MAT-421/blob/main/Module_E.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Continuity and Differentiation\n",
        "## Limits and Continuity\n",
        "\n",
        "We will use the Euclidian norm\n",
        "  $$||\\mathbf{x}||=\\sqrt{\\sum_{i=1}^dx_i^2}$$\n",
        "for $\\mathbf{x}=(x_1,...,x_d)^T$. We also will define an open r-ball around $\\mathbf{x}\\in\\mathbb{R}^d$ which is the set of point distance r from $\\mathbf{x}$\n",
        "  $$B_r(\\mathbf{x})=\\left\\{\\mathbf{y}\\in\\mathbb{R}^d:||\\mathbf{y}-\\mathbf{x}||<r\\right\\}.$$\n",
        "Another definition that we will use is that a point is an accumulation point of a set if for every ball around the point, that at least one other element of the set if contained within. As well, we will use the definition of a limit, \n",
        "$$\\lim_{\\mathbf{x}\\rightarrow\\mathbf{a}}f(\\mathbf{x})=\\mathbf{L}.$$\n",
        "This is related to the definition of continuity:\n",
        "$$\\lim_{\\mathbf{x}\\rightarrow\\mathbf{a}}f(\\mathbf{x})=f(\\mathbf{a}).$$\n",
        "Something else that is useful is that the composite function of two continuous functions is also continuous. That is, if $f(x)$ and $g(x)$ are continious, then $f(g(x))$ is continous if the range of g is a subset of the domain of f.\\\\\n",
        "We also need to define a maximum and minimum, which is that a function M has a maximum at $\\mathbf{z}^*$ if $f(\\mathbf{z}^*)=M$ and $M>f(\\mathbf{x})$ for all x in the domain of f.\n",
        "\n",
        "Another important theorem is that if our function is real valued, continous, and has a domain that is nonemtpy, closed, and bounded, then our function will have a maximum and minimum value.\n",
        "\n",
        "## Derivatives\n",
        "Note that we define a derivative as\n",
        "  $$f'(x_0)=\\frac{df(x_0)}{dx}=\\lim_{x\\to0}\\frac{f(x_0+h)-f(x_0)}{h}$$\n",
        "\n",
        "Note that we will have the following if f and g have derivatives at x\n",
        "$$\\left[\\alpha f(x)+\\beta g(x)\\right]'=\\alpha f'(x)+\\beta g'(x)$$\n",
        "\n",
        "Another thing to note is that at points with derivatives equal to zero, we will find a local mimium or maximum. Another important theorem is the mean value theorem, which is that for two points $a$ and $b$, where $b>a$, then we get that there exists a point $a<c<b$, such that\n",
        "$$f'(c)=\\frac{f(b)-f(a)}{b-a}.$$\n",
        "\n",
        "For our purposes, we will be dealing with multiple independant variables. This,we will define the partial derivative\n",
        "$$\\frac{\\partial f(\\mathbf{x}_0)}{\\partial x_i}=\\lim_{h\\rightarrow0}\\frac{f(\\mathbf{x}_0+h\\mathbf{e}_i)-f(\\mathbf{x}_0)}{h}.$$\n",
        " Suppose that we Note that if we have $\\mathbf{f}=(f_1,...,f_m)$ and that $\\frac{\\partial f_j(\\mathbf{x}_0)}{\\partial x_i}$ exists for all i,j., we get that\n",
        " $$\\mathbf{J}_{\\mathbf{f}}(\\mathbf{x}_0)=\\left(\\begin{array}{ccc}\\frac{\\partial f_1(\\mathbf{x}_0)}{\\partial x_1}&...&\\frac{\\partial f_1(\\mathbf{x}_0)}{\\partial x_d}\\\\...&...&...\\\\\\frac{\\partial f_m(\\mathbf{x}_0)}{\\partial x_1}&...&\\frac{\\partial f_m(\\mathbf{x}_0)}{\\partial x_d}\\end{array}\\right)$$. For a real function, we get that the havobian is the same as the gradiant. Note that the chain rule still applies and that we get\n",
        " $$\\mathbf{J}_{\\mathbf{g}\\circ\\mathbf{f}}(\\mathbf{x}_0) = \\mathbf{J}_{\\mathbf{g}}(\\mathbf{f}(\\mathbf{x}_0))\\mathbf{J}_\\mathbf{f}(\\mathbf{x}_0)$$\n",
        "\n",
        "##Directional Derivative from Gradiant \n",
        "We get that\n",
        "$$\\frac{\\partial f(\\mathbf{x}_0)}{\\partial \\mathbf{v}}=\\nabla f(\\mathbf{x}_0)^T\\mathbf{v}.$$\n",
        "There is a second derivative equivalent of the Jacobian, the Hessian\n",
        "$$\\mathbf{H}_{\\mathbf{f}}(\\mathbf{x}_0)=\\left(\\begin{array}{ccc}\\frac{\\partial^2 f_1(\\mathbf{x}_0)}{\\partial x_1^2}&...&\\frac{\\partial^2 f_1(\\mathbf{x}_0)}{\\partial x_d^2}\\\\...&...&...\\\\\\frac{\\partial^2 f_m(\\mathbf{x}_0)}{\\partial x_1^2}&...&\\frac{\\partial^2 f_m(\\mathbf{x}_0)}{\\partial x_d^2}\\end{array}\\right)$$. \n",
        "\n",
        "## Taylor's Theorem\n",
        "\n",
        "Taylor's Theorem states that we get\n",
        "$$f(b)=f(a)+(b-a)f'(a)+\\frac{1}{2}(b-a)^2f''(a)+...+\\frac{(b-a)^{m-1}}{(m-1)!}f^{(m-1)}(a)+...$$\n",
        "\n",
        "#Opzimization\n",
        "\n",
        "We are interestd of finding an optimizer of the form\n",
        "$$\\lim_{\\mathbf{x}\\in\\mathbb{R}^d}f(\\mathbf{x}).$$\n",
        "We now differentiate between a global and a local minimizer. A global minimuzer is a point that is smaller than all other points in the domain, while a local minimizer is smaller than other points in a neighborhood around it. \n",
        "\n",
        "A useful concept is that of a decent direction. A vector $\\mathbf{v}$ is a descent direction of the function f if there exists some $\\alpha^*>0$ such that\n",
        "$$f(\\mathbf{x}+\\alpha^*\\mathbf{v})<f(\\mathbf{x}_0),\\forall\\alpha\\in(0,\\alpha^*).$$\n",
        "We also find that it is also the case that $\\mathbf{x}$ is a descent direction if\n",
        "$$\\frac{\\partial f(\\mathbf{x}_0)}{\\partial \\mathbf{v}}=\\nabla f(\\mathbf{x}_0)^T\\mathbf{v}<0.$$\n",
        "Something else that we can find is that if a function does not have a gradiant of zero, then it will have a descent direction. \n",
        "\n",
        "We know that for a single variable function that we have a local extrema whenever the derivative of the function is equal to zero at that point. This also turns out to be a condition for a local extremea in a multi-variable function:\n",
        "  $$\\nabla f(\\mathbf{x}_0)=0.$$\n",
        " \n",
        "We also introduce the definition of positive semi definite, which is a $d\\times d$ matrix $H$ for which $\\mathbf{x}^TH\\mathbf{x}\\geq0$ for all $\\mathbf{x}\\in\\mathbb{R}^d$. With this definition, we also get that if $\\mathbf{x}_0$ is a local minimizer of f if $\\mathbf{H}_f(\\mathbf{x}_0)$ is PSD.  \n",
        "\n",
        "Now, we can get to a stric definition of a local minimizer, which is that\n",
        "$$\\nabla f(\\mathbf{x}_0)=0 \\text{ and }\\mathbf{H}_f(\\mathbf{x}_0)\\text{ is PSD}$$\n",
        "\n",
        "## Convexity\n",
        "One thing to note is that the conditions that we have found can only find local minimizers, as they only care about the local behaviour of the function. To find global minimizers, we will care about convexity. A convex set is a set such that every point on a line connecting any two points in the set is also contained within the set. That is, that for $\\mathbf{x},\\mathbf{y}\\in D$, that for all $\\alpha\\in[0,1]$,\n",
        "  $$(1-\\alpha)\\mathbf{x}+\\alpha\\mathbf{y}\\in D.$$\n",
        "We also care about convex functions, in which we define a function as convex if\n",
        "  $$f((1-\\alpha)\\mathbf{x}+\\alpha\\mathbf{y})\\leq(1-\\alpha)f(\\mathbf{x})+\\alpha f(\\mathbf{y})$$\n",
        "for all $\\mathbf{x},\\mathbf{y}\\in \\mathbb{R}^d$ and $\\alpha\\in[0,1]$.One thing to note is that all affline functions, which are of the form $f(\\mathbf{x})=\\mathbf{w}^T\\mathbf{x}+b$, are convex. We will now tru to show that a multibariable function is convex. We will note that a function f is convex if and only if\n",
        "$$f(\\mathbf{y})\\geq f(\\mathbf{x})+\\nabla f(\\mathbf{x})^T(\\mathbf{y}-\\mathbf{x}).$$\n",
        " As well, we can formulate a second order version, which is that f is convex if and only if for all $\\mathbf{x}\\in\\mathbb{R}^d$, then $\\mathbf{H}_f(\\mathbf{x})$ is PSD.\n",
        "\n",
        "For convex functions, we get that $\\mathbf{x}_0$ is convex if and only if $\\nabla f(\\mathbf{x}_0)=0$. Something about convex function is that any minimizer is also a global minimizer. \n",
        "\n",
        "To find $\\mathbf{x}$ that minimizes $f(\\mathbf{x})$, we can employ a variety of methods. One such method would be to find the stationary points of the function and then find which of these causes the smallest output of the function. This would be nice, however, we may reach systems of nonlinear equations that do not have explicity solutions, so we will have to develop other methods. \n",
        "\n",
        "Another method is the steepest descent method, in which we follow the direction in which f decreases to find a mimimum. This is done iteratively in the form\n",
        "  $$\\mathbf{x}^{k+1}=\\mathbf{x}^k-\\alpha_k\\nabla(\\mathbf{x}^k)$$\n",
        "for $k=0,1,2,...,$ where $\\alpha_k$ is the step length. We choose this step length to be\n",
        "$$\\alpha_k = \\arg\\min_{\\alpha>0}f(\\mathbf{x}^k-\\alpha\\nabla f(\\mathbf{x}^k)).$$\n",
        "From this method, we get that each next point will be smaller than the previous."
      ],
      "metadata": {
        "id": "Hdgt2YQWIWJy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PjSpgQG1z7mz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}